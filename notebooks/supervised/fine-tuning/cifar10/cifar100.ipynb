{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oW6MoHmLAjVG"
   },
   "source": [
    "# Supervised Fine-Tuning Cifar10\n",
    "\n",
    "Code: [github:lucasdavid/experiments/.../supervised/fine-tuning/cifar10](https://github.com/lucasdavid/experiments/blob/main/notebooks/supervised/fine-tuning/cifar10/cifar10.ipynb)  \n",
    "Dataset: Cifar10  \n",
    "Docker image: `tensorflow/tensorflow:latest-gpu-jupyter`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import tensorflow as tf\n",
    "\n",
    "class RC:\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    seed = 5131\n",
    "\n",
    "class DC:\n",
    "    batch_size = 64\n",
    "    image_size = (32, 32)\n",
    "    channels = 3\n",
    "    input_shape = (batch_size, *image_size, channels)\n",
    "\n",
    "class TC:\n",
    "    epochs = 200\n",
    "    learning_rate = .001\n",
    "    \n",
    "    epochs_fine_tuning = 0\n",
    "    learning_rate_fine_tuning = .0005\n",
    "\n",
    "    validation_split = '30%'\n",
    "    reduce_lr_on_plateau_factor = .5\n",
    "\n",
    "    splits = [f'train[{validation_split}:]', f'train[:{validation_split}]', 'test']\n",
    "    \n",
    "    augment = False\n",
    "\n",
    "class LogConfig:\n",
    "    tensorboard = (f'/tf/logs/d:cifar100 '\n",
    "                   f'e:{TC.epochs} b:{DC.batch_size} v:{TC.validation_split} '\n",
    "                   f'm:mobilenetv2 aug:{TC.augment} sd:{RC.seed}'\n",
    "                   f'/{int(time())}')\n",
    "    \n",
    "class Config:\n",
    "    run = RC\n",
    "    data = DC\n",
    "    training = TC\n",
    "    log = LogConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6178,
     "status": "ok",
     "timestamp": 1596906675688,
     "user": {
      "displayName": "Lucas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgO-xqdSSwbB8xXos7Shqhpq7QJ06mYVUvdLDRcbQ=s64",
      "userId": "01670254553878112810"
     },
     "user_tz": 180
    },
    "id": "qo_IhudYAnF7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from math import ceil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import Model, Sequential, Input\n",
    "from tensorflow.keras.layers import (Conv2D, Dense, Dropout, BatchNormalization,\n",
    "                                     Activation, Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(y, titles=None, rows=1, i0=0):\n",
    "    for i, image in enumerate(y):\n",
    "        if image is None:\n",
    "            plt.subplot(rows, ceil(len(y) / rows), i0+i+1)\n",
    "            plt.axis('off')\n",
    "            continue\n",
    "\n",
    "        t = titles[i] if titles else None\n",
    "        plt.subplot(rows, ceil(len(y) / rows), i0+i+1, title=t)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ut3PgruYAkp_"
   },
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    (train_ds, val_ds, test_ds), info = tfds.load(\n",
    "    'cifar100',\n",
    "    split=Config.training.splits,\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True)\n",
    "\n",
    "    class_names = np.asarray(info.features['label'].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Data.info.citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchwise_augmentation = Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom((-.3, .3)),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "], name='batch_aug')\n",
    "\n",
    "def augment_fn(image, label):\n",
    "    image = samplewise_augmentation(image)\n",
    "    image = tf.clip_by_value(image, 0, 255)\n",
    "    return image, label\n",
    "\n",
    "def prepare(ds):\n",
    "    ds = ds.batch(Config.data.batch_size, drop_remainder=True)\n",
    "    return ds.prefetch(buffer_size=Config.run.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = prepare(Data.train_ds)\n",
    "val_ds = prepare(Data.val_ds)\n",
    "test_ds = prepare(Data.test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_ds:\n",
    "    print('Shapes:', x.shape, 'and', y.shape)\n",
    "    print(\"Labels: \", y.numpy())\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plot(x.numpy().astype(int), rows=4)\n",
    "    plt.tight_layout()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import mobilenet_v2\n",
    "\n",
    "encoder = mobilenet_v2.MobileNetV2(include_top=False, pooling='avg',\n",
    "                                   input_shape=Config.data.input_shape[1:])\n",
    "encoder = Model(encoder.input, encoder.get_layer('block_9_add').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_pre(x):\n",
    "    return Lambda(mobilenet_v2.preprocess_input, name='pre_inception')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "def dense_block(x, units, activation='relu', name=None):\n",
    "    y = Dense(units, name=f'{name}_fc', use_bias=False)(x)\n",
    "    y = BatchNormalization(name=f'{name}_bn')(y)\n",
    "    y = Activation(activation, name=f'{name}_relu')(y)\n",
    "    return y\n",
    "    \n",
    "def discriminator():\n",
    "    y = x = Input(shape=Config.data.input_shape[1:], name='inputs')\n",
    "    if Config.training.augment:\n",
    "        y = batchwise_augmentation(y)\n",
    "    y = encoder_pre(y)\n",
    "    y = encoder(y)\n",
    "    y = GlobalAveragePooling2D(name='avg')(y)\n",
    "    y = Dense(len(Data.class_names), name='predictions')(y)\n",
    "    return tf.keras.Model(x, y, name='author_disc')\n",
    "\n",
    "disc = discriminator()\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.get_layer('model').trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(disc, show_shapes=True, show_dtype=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, metrics, optimizers\n",
    "\n",
    "disc.compile(\n",
    "    optimizer=optimizers.Adam(lr=Config.training.learning_rate),\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        metrics.SparseCategoricalAccuracy(),\n",
    "        metrics.SparseTopKCategoricalAccuracy()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Training for Final Classification Layer\n",
    "\n",
    "The final layer --- currently containing random values --- must be first adjusted to match the the encoder's layers' current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "callbacks = [\n",
    "    callbacks.TerminateOnNaN(),\n",
    "    callbacks.ModelCheckpoint(Config.log.tensorboard + '/weights.h5',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1),\n",
    "    callbacks.ReduceLROnPlateau(patience=Config.training.epochs // 2,\n",
    "                                factor=Config.training.reduce_lr_on_plateau_factor),\n",
    "    callbacks.EarlyStopping(patience=Config.training.epochs // 3),\n",
    "    callbacks.TensorBoard(Config.log.tensorboard, histogram_freq=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=Config.training.epochs,\n",
    "    initial_epoch=0,\n",
    "    callbacks=callbacks,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.training.epochs_fine_tuning:\n",
    "    disc.get_layer('model').trainable = True\n",
    "\n",
    "    disc.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        initial_epoch=disc.history.epoch[-1] + 1,\n",
    "        epochs=len(disc.history.epoch) + Config.training.epochs_fine_tuning,\n",
    "        callbacks=callbacks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.get_layer('model').trainable = False\n",
    "\n",
    "disc.load_weights(Config.log.tensorboard + '/weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "def labels_and_predictions(model, ds):\n",
    "    labels, predictions = [], []\n",
    "    \n",
    "    for x, y in ds:\n",
    "        p = model(x).numpy()\n",
    "        p = p.argmax(axis=1)\n",
    "        \n",
    "        labels.append(y.numpy())\n",
    "        predictions.append(p)\n",
    "    \n",
    "    labels, predictions = np.concatenate(labels), np.concatenate(predictions)\n",
    "    labels, predictions = Data.class_names[labels], Data.class_names[predictions]\n",
    "    return labels, predictions\n",
    "\n",
    "def evaluate(model, ds):\n",
    "    labels, predictions = labels_and_predictions(model, ds)\n",
    "    \n",
    "    print('balanced acc:', skmetrics.balanced_accuracy_score(labels, predictions))\n",
    "    print('accuracy    :', skmetrics.accuracy_score(labels, predictions))\n",
    "    print('Classification report:')\n",
    "    print(skmetrics.classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(disc, train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(disc, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(disc, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, predictions = labels_and_predictions(disc, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = skmetrics.confusion_matrix(labels, predictions)\n",
    "sorted_by_most_accurate = (cm / cm.sum(axis=1, keepdims=True)).diagonal().argsort()[::-1]\n",
    "cm = cm[sorted_by_most_accurate][:, sorted_by_most_accurate]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(cm,\n",
    "            cmap='RdPu', annot=False, cbar=False,\n",
    "            yticklabels=Data.class_names[sorted_by_most_accurate],\n",
    "            xticklabels=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, ds, take=1):\n",
    "    figs, titles = [], []\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    for ix, (x, y) in enumerate(ds.take(take)):\n",
    "        p = model.predict(x).argmax(axis=-1)\n",
    "        \n",
    "        figs.append(x.numpy())\n",
    "        titles.append([f'{a} {b}' for a, b in zip(y, p)])\n",
    "        \n",
    "    plot(np.concatenate(figs),\n",
    "         titles=sum(titles, []),\n",
    "         rows=6)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_predictions(disc, train_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

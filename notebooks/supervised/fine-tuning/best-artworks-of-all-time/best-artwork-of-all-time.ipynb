{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oW6MoHmLAjVG"
   },
   "source": [
    "# Supervised Fine-Tuning Best Artworks of All Time\n",
    "\n",
    "Code: [github:lucasdavid/experiments/.../supervised/fine-tuning/best-artworks-of-all-time](https://github.com/lucasdavid/experiments/blob/main/notebooks/supervised/fine-tuning/best-artworks-of-all-time/best-artworks-of-all-time.ipynb)  \n",
    "Dataset: https://www.kaggle.com/ikarus777/best-artworks-of-all-time  \n",
    "Docker image: `tensorflow/tensorflow:latest-gpu-jupyter`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import tensorflow as tf\n",
    "\n",
    "class RC:\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    seed = 21392\n",
    "\n",
    "class DC:\n",
    "    path = '/tf/datasets/best-artworks-of-all-time'\n",
    "    images = path + '/images/images'\n",
    "    info = path + '/artists.csv'\n",
    "\n",
    "    batch_size = 32\n",
    "    image_size = (299, 299)\n",
    "    channels = 3\n",
    "    input_shape = (batch_size, *image_size, channels)\n",
    "\n",
    "    buffer_size = 100000\n",
    "\n",
    "class TC:\n",
    "    epochs = 200\n",
    "    learning_rate = .001\n",
    "    validation_split = .3\n",
    "    reduce_lr_on_plateau_pacience = 20\n",
    "    reduce_lr_on_plateau_factor = .5\n",
    "    \n",
    "    early_stopping_patience = 50\n",
    "    \n",
    "    splits = [f'train[{validation_split}:]', f'train[:{validation_split}]', 'test']\n",
    "    \n",
    "    augment = True\n",
    "    \n",
    "    epochs_fine_tuning = 0\n",
    "    learning_rate_fine_tuning = .0005\n",
    "    fine_tuning_layers = .2  # 20%\n",
    "    \n",
    "class LogConfig:\n",
    "    tensorboard = (f'/tf/logs/d:baoat '\n",
    "                   f'e:{TC.epochs} fte:{TC.epochs_fine_tuning} b:{DC.batch_size} '\n",
    "                   f'v:{TC.validation_split} m:inceptionv3 aug:{TC.augment}'\n",
    "                   f'/{int(time())}')\n",
    "    \n",
    "class Config:\n",
    "    run = RC\n",
    "    data = DC\n",
    "    training = TC\n",
    "    log = LogConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6178,
     "status": "ok",
     "timestamp": 1596906675688,
     "user": {
      "displayName": "Lucas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgO-xqdSSwbB8xXos7Shqhpq7QJ06mYVUvdLDRcbQ=s64",
      "userId": "01670254553878112810"
     },
     "user_tz": 180
    },
    "id": "qo_IhudYAnF7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from math import ceil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import Model, Sequential, Input\n",
    "from tensorflow.keras.layers import (Conv2D, Dense, Dropout, BatchNormalization,\n",
    "                                     Activation, Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(y, titles=None, rows=1, i0=0):\n",
    "    for i, image in enumerate(y):\n",
    "        if image is None:\n",
    "            plt.subplot(rows, ceil(len(y) / rows), i0+i+1)\n",
    "            plt.axis('off')\n",
    "            continue\n",
    "\n",
    "        t = titles[i] if titles else None\n",
    "        plt.subplot(rows, ceil(len(y) / rows), i0+i+1, title=t)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ut3PgruYAkp_"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.data.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    info = pd.read_csv(Config.data.info)\n",
    "    data_dir = pathlib.Path(Config.data.images)\n",
    "\n",
    "    class_names = np.array(sorted([item.name\n",
    "                           for item in data_dir.glob('*')\n",
    "                           if item.name != \"LICENSE.txt\"]))\n",
    "    \n",
    "    dataset_args = dict(\n",
    "        label_mode='int',\n",
    "        image_size=Config.data.image_size, batch_size=Config.data.batch_size,\n",
    "        validation_split=Config.training.validation_split,\n",
    "        seed=Config.run.seed)\n",
    "\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        subset='training',\n",
    "        **dataset_args)\n",
    "    \n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        subset='validation',\n",
    "        **dataset_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchwise_augmentation = Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom((-.3, .3)),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "], name='batch_aug')\n",
    "\n",
    "def augment_fn(image, label):\n",
    "    image = batchwise_augmentation(image)\n",
    "    image = tf.clip_by_value(image, 0, 255)\n",
    "    return image, label\n",
    "\n",
    "def prepare(ds, augment=False):\n",
    "    if augment: ds = ds.map(augment_fn, num_parallel_calls=Config.run.AUTOTUNE)\n",
    "    return ds.prefetch(buffer_size=Config.run.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = prepare(Data.train_ds, augment=Config.training.augment)\n",
    "val_ds = prepare(Data.val_ds)\n",
    "# test_ds = prepare(Data.test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_ds:\n",
    "    print('Shapes:', x.shape, 'and', y.shape)\n",
    "    print(\"Labels: \", y.numpy())\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plot(x.numpy().astype(int), rows=4)\n",
    "    plt.tight_layout()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import inception_resnet_v2\n",
    "\n",
    "encoder = inception_resnet_v2.InceptionResNetV2(include_top=False, pooling='avg',\n",
    "                                                input_shape=Config.data.input_shape[1:])\n",
    "# encoder = Model(encoder.input, encoder.get_layer('block_9_add').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_pre(x):\n",
    "    return Lambda(inception_resnet_v2.preprocess_input, name='pre_incresnet')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "def dense_block(x, units, activation='relu', name=None):\n",
    "    y = Dense(units, name=f'{name}_fc', use_bias=False)(x)\n",
    "    y = BatchNormalization(name=f'{name}_bn')(y)\n",
    "    y = Activation(activation, name=f'{name}_relu')(y)\n",
    "    return y\n",
    "    \n",
    "def discriminator():\n",
    "    y = x = Input(shape=Config.data.input_shape[1:], name='inputs')\n",
    "    y = encoder_pre(y)\n",
    "    y = encoder(y)\n",
    "    y = Dense(len(Data.class_names), name='predictions')(y)\n",
    "    return tf.keras.Model(x, y, name='author_disc')\n",
    "\n",
    "disc = discriminator()\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.get_layer('inception_resnet_v2').trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, metrics, optimizers\n",
    "\n",
    "disc.compile(\n",
    "    optimizer=optimizers.Adam(lr=Config.training.learning_rate),\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        metrics.SparseCategoricalAccuracy(),\n",
    "        metrics.SparseTopKCategoricalAccuracy()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Training for Final Classification Layer\n",
    "\n",
    "The final layer --- currently containing random values --- must be first adjusted to match the the encoder's layers' current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "try:\n",
    "    disc.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=Config.training.epochs,\n",
    "        initial_epoch=0,\n",
    "        callbacks=[\n",
    "            callbacks.TerminateOnNaN(),\n",
    "            callbacks.ModelCheckpoint(Config.log.tensorboard + '/weights.h5',\n",
    "                                      save_best_only=True,\n",
    "                                      save_weights_only=True,\n",
    "                                      verbose=1),\n",
    "            callbacks.ReduceLROnPlateau(patience=Config.training.reduce_lr_on_plateau_pacience,\n",
    "                                        factor=Config.training.reduce_lr_on_plateau_factor,\n",
    "                                        verbose=1),\n",
    "            callbacks.EarlyStopping(patience=Config.training.early_stopping_patience, verbose=1),\n",
    "            callbacks.TensorBoard(Config.log.tensorboard, histogram_freq=1)\n",
    "        ]);\n",
    "except KeyboardInterrupt:\n",
    "    print('stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.load_weights(Config.log.tensorboard + '/weights.h5')\n",
    "\n",
    "disc.get_layer('inception_resnet_v2').trainable = True\n",
    "disc.save_weights(Config.log.tensorboard + '/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.training.epochs_fine_tuning:\n",
    "    _enc = disc.get_layer('inception_resnet_v2')\n",
    "    ft_layer_ix = int((1-Config.training.fine_tuning_layers)*len(_enc.layers))\n",
    "    \n",
    "    for ix, l in enumerate(_enc.layers):\n",
    "        l.trainable = ix >= ft_layer_ix\n",
    "\n",
    "    try: disc.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        initial_epoch=disc.history.epoch[-1] + 1,\n",
    "        epochs=len(disc.history.epoch) + Config.training.epochs_fine_tuning,\n",
    "        callbacks=[\n",
    "            callbacks.TerminateOnNaN(),\n",
    "            callbacks.ModelCheckpoint(Config.log.tensorboard + '/weights.h5',\n",
    "                                      save_best_only=True,\n",
    "                                      save_weights_only=True,\n",
    "                                      verbose=1),\n",
    "            callbacks.ReduceLROnPlateau(patience=Config.training.reduce_lr_on_plateau_pacience,\n",
    "                                        factor=Config.training.reduce_lr_on_plateau_factor,\n",
    "                                        verbose=1),\n",
    "            callbacks.EarlyStopping(patience=Config.training.early_stopping_patience,\n",
    "                                    verbose=1),\n",
    "            callbacks.TensorBoard(Config.log.tensorboard, histogram_freq=1)\n",
    "        ]);\n",
    "    except KeyboardInterrupt: print('stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.training.epochs_fine_tuning:\n",
    "    disc.load_weights(Config.log.tensorboard + '/weights.h5')\n",
    "\n",
    "    disc.get_layer('inception_resnet_v2').trainable = True\n",
    "    disc.save_weights(Config.log.tensorboard + '/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.load_weights(Config.log.tensorboard + '/weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "def labels_and_predictions(model, ds):\n",
    "    labels, predictions = [], []\n",
    "    \n",
    "    for x, y in ds:\n",
    "        p = model(x).numpy()\n",
    "        p = p.argmax(axis=1)\n",
    "        \n",
    "        labels.append(y.numpy())\n",
    "        predictions.append(p)\n",
    "    \n",
    "    labels, predictions = np.concatenate(labels), np.concatenate(predictions)\n",
    "    labels, predictions = Data.class_names[labels], Data.class_names[predictions]\n",
    "    return labels, predictions\n",
    "\n",
    "def evaluate(model, ds):\n",
    "    labels, predictions = labels_and_predictions(model, ds)\n",
    "    \n",
    "    print('balanced acc:', skmetrics.balanced_accuracy_score(labels, predictions))\n",
    "    print('accuracy    :', skmetrics.accuracy_score(labels, predictions))\n",
    "    print('Classification report:')\n",
    "    print(skmetrics.classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(disc, train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(disc, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = val_ds\n",
    "\n",
    "evaluate(disc, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, predictions = labels_and_predictions(disc, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = skmetrics.confusion_matrix(labels, predictions)\n",
    "cm = cm / cm.sum(axis=1, keepdims=True)\n",
    "sorted_by_most_accurate = cm.diagonal().argsort()[::-1]\n",
    "cm = cm[sorted_by_most_accurate][:, sorted_by_most_accurate]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "with sns.axes_style(\"white\"):\n",
    "    sns.heatmap(cm, cmap='RdPu', annot=False, cbar=False,\n",
    "                yticklabels=Data.class_names[sorted_by_most_accurate], xticklabels=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, ds, take=1):\n",
    "    figs, titles = [], []\n",
    "    \n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for ix, (x, y) in enumerate(ds.take(take)):\n",
    "        p = model.predict(x)\n",
    "        p = tf.nn.softmax(p).numpy()\n",
    "        figs.append(x.numpy().astype(int))\n",
    "        titles.append([f'label: {a}\\npredicted: {b}\\nproba:{c:.0%}'\n",
    "                       for a, b, c in zip(Data.class_names[y],\n",
    "                                          Data.class_names[p.argmax(axis=-1)],\n",
    "                                          p.max(axis=-1))])\n",
    "    plot(np.concatenate(figs),\n",
    "         titles=sum(titles, []),\n",
    "         rows=6)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_predictions(disc, train_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

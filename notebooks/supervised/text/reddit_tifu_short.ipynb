{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reddit_tifu/short\n",
    "\n",
    "Dataset: tensorflow/datasets/reddit_tifu/short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DC:\n",
    "    dataset = 'reddit_tifu/short'\n",
    "    split = ['train[:50%]', 'train[50%:70%]', 'train[70%:]']\n",
    "    batch_size = 64\n",
    "\n",
    "class MC:\n",
    "    vocab_size = 10000\n",
    "    embedding_features = 8\n",
    "    sequence_length = 100\n",
    "    \n",
    "class Config:\n",
    "    data = DC\n",
    "    model = MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, re, shutil, string\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sample_fn(s):\n",
    "    return (s['documents'], s['tldr'])\n",
    "\n",
    "def standardize_fn(x):\n",
    "    x = tf.strings.lower(x)\n",
    "    return tf.strings.regex_replace(x, '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "def prepare(ds):\n",
    "    return (ds.filter(lambda r: r['score'] != '')\n",
    "              .batch(Config.data.batch_size)\n",
    "              .map(extract_sample_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "              .cache()\n",
    "              .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "class Data:\n",
    "    (train, val, test), info = tfds.load(Config.data.dataset,\n",
    "                                         split=Config.data.split,\n",
    "                                         with_info=True,\n",
    "                                         shuffle_files=True)\n",
    "    \n",
    "    (train, val, test) = map(prepare, (train, val, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=standardize_fn,\n",
    "    max_tokens=Config.model.vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=Config.model.sequence_length)\n",
    "\n",
    "text_ds = Data.train.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "em = Embedding(\n",
    "    Config.model.vocab_size,\n",
    "    Config.model.embedding_features,\n",
    "    name='em')\n",
    "\n",
    "score_model = Sequential([\n",
    "    vectorize_layer,\n",
    "    em,\n",
    "    GlobalAveragePooling1D(name='avg_pool'),\n",
    "    Dense(16, activation='relu', name='fc1'),\n",
    "    Dense(1, name='predictions')],\n",
    "    name='score_reg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
